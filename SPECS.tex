% Created 2025-02-22 Sat 10:08
% Intended LaTeX compiler: pdflatex
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\renewcommand*\familydefault{\sfdefault}
\setlength{\parindent}{0pt}
\setlength{\parskip}{3pt}
\usepackage[left=1.5cm,right=1cm,top=1cm,bottom=1cm]{geometry}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{dirtree}
\author{Victor C, Flor De Luz}
\date{2024-05-20}
\title{DATA ANALYSIS AND VISUALIZATION OF MULTIVARIATE TIME SERIES SOFTWARE}
\hypersetup{
 pdfauthor={Victor C, Flor De Luz},
 pdftitle={DATA ANALYSIS AND VISUALIZATION OF MULTIVARIATE TIME SERIES SOFTWARE},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.3 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Introduction}
\label{sec:org70c6fdc}

The project explores, analyzes, and integrates Vue.JS and Python code with different models for clean and data completion.

\section{Goals}
\label{sec:org6aac51a}

\begin{itemize}
\item Complete the code by integrating different Python models into the software project.

\item Data structure improvement to adapt new ML models for cleaning data and completion.

\item Technical documentation of the software.
\end{itemize}

\section{State of the art}
\label{sec:org9d851e0}

\begin{itemize}
\item Source code of the project(1), with documentation to install and deploy the software. The software currently reads data and graphs time series. It has options for data completion through rolling mean and kNN and partially cleaning data alternatives. It tracks for changes while navigating through the Diagram Operator interface graphically. Integrates Radial Chart for time series cycles.

\item Self-documented notebook with multiple machine-learning techniques and their variants for data completion like Rolling Mean, Decision Trees, Stochastic Grading Boosting, Locally Weighted Regression, Legendre Polynomials Regression, Random Forest Regressor, k-nearest Neighbors. Includes removing features with no data at all. Computes a Dicky Fuller Stationarity Test. Automatically computes Weighted MAPE and R-Score + RMSE to detect and suggest which model fits better. Computes Autocorrelation, Fourier, and Hodrick Prescott to detect Cyclicity.
\end{itemize}

\section{Pre-implementation analysis}
\label{sec:org14ce080}

\subsection{Multilayer design}
\label{sec:org4d094bd}

The critical aspects of the software are:

\begin{itemize}
\item Frontend developed in Vue.JS

\item Backend developed in Python

\item The data source layer primarily are CSV files \footnote{HDF5 detected}

\begin{center}
\includesvg[width=.9\linewidth]{link-data-069cc0f38b52fd29f3cd19de9f17d6d1e8ec9344.edraw}
\end{center}
\end{itemize}

\subsection{Frontend specifics}
\label{sec:org762717c}

\begin{center}
\includesvg[width=.9\linewidth]{link-data-1250ef5082f07210935c7515e20cafdd9150c370.edraw}
\end{center}

\subsection{Backend specifics}
\label{sec:org8f83290}

\begin{center}
\includesvg[width=.9\linewidth]{link-data-2de380ab2182abb59bc5043236fa1864057da655.edraw}
\end{center}

\subsection{Detected issues}
\label{sec:orge4f1650}

\begin{itemize}
\item Data structures. It requires adapting the current data structures of the program to the new dictionary of the tested and trained models.

\item Exceptional cases. There are some treatments in the frontend code for specific data sources \footnote{TimeSeries.vue, temperature, and precipitation, line 308}. Those treatments will done in the backend.

\item The Radial Diagram component \footnote{Spiral.vue} hardcode for temperature and precipitation. We will rewrite the code to accept different datasets and features. Additionally, it is currently consuming the old backend\footnote{api/main.py}. The old backend has to be re-implemented in the new backend\footnote{new\_api/main.py}.
\end{itemize}

\section{Implementation}
\label{sec:org0b354f1}

\subsection{Activities summary}
\label{sec:org76f7092}

\begin{itemize}
\item Adapt the Backend Data Structure.

\item Data Structure Integration. It will include saving a dictionary and allowing notebook compatibility.

\item Adapt the Frontend Data Structure.

\item Implement new ML models in the back end.

\item Adapt the front end for new ML models.

\item Frontend UI additional improvements.

\item Format multivariate time series data.
\end{itemize}


\subsection{Activities execution}
\label{sec:org769784a}

\subsubsection{Change log}
\label{sec:orgf018222}

Tracking changes on the program will be done by using diff to create a patch component that allows the creation of a checkpoint and registering the changes by the size of modify or created code\footnote{Tracking log in the appendix}. The folder structure is:

\begin{longtable}{lll}
\caption{Change log folder structure}
\\[0pt]
\textbf{A} & \textbf{File System Structure} & \textbf{Description}\\[0pt]
\hline
\endfirsthead
\multicolumn{3}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{A} & \textbf{File System Structure} & \textbf{Description} \\[0pt]

\hline
\endhead
\hline\multicolumn{3}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
drwx & VisWeb-AlgoritmosLimpieza.incr & Incremental Checkpoint\\[0pt]
drwx & VisWeb-AlgoritmosLimpieza.diff & Diff/Patch Repository\\[0pt]
drwx & VisWeb-AlgoritmosLimpieza & Development Folder\\[0pt]
-r-- & checkpoint.lisp & The program for recording changes\\[0pt]
\end{longtable}

\subsubsection{Backlog}
\label{sec:orgcfe7f4e}

The pending activities are in the appendix backlog section. These activities correspond to changes or reviews that depend on multiple program files around the software, and their resolutions will come on the project's timeline. Bugs will gradually fixed.

\subsubsection{Backend data structures}
\label{sec:orgfa76442}

Programs adaptation in the new\_api.

\begin{itemize}
\item Creating a new MainloaderClass and its derived GenLoader class as a generic data loader that extends MainloaderClass\footnote{The new Class defines the notebook's data structure}. Methods' names remain unchanged to keep compatibility with the original code. \textbf{Created}.
\end{itemize}

\large
\begin{center}
\includesvg[width=.9\linewidth]{link-data-b109f550576e09ad99add7f1f39939c296d2b388.edraw}
\end{center}
\normalsize

\begin{itemize}
\item The derived class GenLoader improves:

\begin{itemize}
\item The differences in the data structures returned by some Data Loaders. Those differences crash the Python kernel. \textbf{Fixed}.

\item Some old Classes' methods trunk data to 1000 rows for interprocess communication. \textbf{Fixed}.
\end{itemize}

\item Normalization will use MaxAbsScaler. Max scaler detected without considering negative values in some normalizations through the \emph{main.py} in the new\_api code. \textbf{Fixed}.

\item The GenLoader-derived class allows different datasets to load. It returns a new loader object with the needed structure. It loads the data from the source when the constructor creates an instance of the class. The main program will map correctly the different datasets when the front end requires it. \textbf{Implemented}.

\item The front end becomes slow with too much data. A resample by day solves the issue. Internally, it will keep the original data, and it will transform the data without resampling. The resamples are just for visualization purposes. \textbf{Fixed}.

\item Code refactoring is mandatory\footnote{Testing functions in the appendix}. Some functions have no real input for data observations and use a random dataset created inside the function. Other functions presented incomplete treatment. Dimensionality reduction rewritten functions (2). \textbf{Implemented}.

\item Duplicated and non-relevant functions have been removed. \textbf{Fixed}.

\item The transformations will be packed in the MainredoClass and its derived class GenRedo, where the methods will be different techniques to clean and complete data. \textbf{Created}.
\end{itemize}

\large
\begin{center}
\includesvg[width=.9\linewidth]{link-data-41552ea8ba76b31fd563f0a168fb2c7cc54f6954.edraw}
\end{center}
\normalsize

\subsubsection{Backend new specifications}
\label{sec:org0d2a09e}

\begin{enumerate}
\item Classes and modules
\label{sec:org5431e80}

\begin{center}
\includesvg[width=.9\linewidth]{link-data-7390a9932dd2cb3129ada2c263fda3d0172031ac.edraw}
\end{center}

\item Sequence diagram
\label{sec:org2c9fcd2}

\small
\begin{center}
\includesvg[width=.9\linewidth]{link-data-abeb00b5b06a1ab332d0fe6ab7b8bcf777f1b970.edraw}
\end{center}
\normalsize
\end{enumerate}

\subsubsection{Backend new data structure and frontend integration}
\label{sec:org6d2231c}

The initial state of the front end had two APIs. All the methods to grab data were migrated and transformed in the new API, leaving just one centralized processing Python API.

The main structure of the front end has not changed too much. The most relevant changes were in coding because the program was created for Peruvian temperature and precipitation datasets.

\begin{center}
\includesvg[width=.9\linewidth]{link-data-2622180df0d470ec8299135efd823dc950ac899d.edraw}
\end{center}

\subsubsection{Data repository and cache}
\label{sec:org96ad333}

The backend will support dataset uploading. The structure has been defined on folders as an ID inside a .data repository. It will allow us to keep the data treatment progress in folders with the same ID inside a .cache repository.

\section{Requirements}
\label{sec:org71317b5}

\subsection{Software}
\label{sec:org1ea8b40}

\begin{itemize}
\item Python 3.12.9

\item NodeJS 18.19.1

\item GNU/Linux distribution with kernel 5.15.19 or higher

\item Windows has not been tested, but it may work through Anaconda
\end{itemize}

\subsection{Minimum Hardware}
\label{sec:orgdb10fe5}

\begin{itemize}
\item Processor AMD64 or x86\_64 architecture

\item 8GB RAM

\item 32GB SSD SWAP

\item 64GB HD (SSD recommended)

\item GPU (Optional)
\end{itemize}

\section{Visual Analytics Guidance Development}
\label{sec:org1e56fa9}

\subsection{Spiral diagram analysis improvement}
\label{sec:org60cbf03}

\begin{itemize}
\item Compute segments and timespan for the Spiral. \textbf{Done}.

\item Multiple time series integration. \textbf{Done}.

\item Fix polygon coordinates when there are more than three dimensions. \textbf{Done}.
\end{itemize}

\subsection{Independent section for statistics}
\label{sec:org9b384d0}

\begin{itemize}
\item Statistics of the time series (size, nulls, type of columns, or dimensions). \textbf{Done}.

\item Distribution type of the time series (e.g., normal). \textbf{Done}.

\item Outliers. \textbf{Done}.

\item Correlation matrix. \textbf{Done}.
\end{itemize}

\subsection{Flow diagram improvement}
\label{sec:org76850a2}

\begin{itemize}
\item Data cleaning, Normalization and transformation, and Time-series behavior. \textbf{Done}.

\item Stoppers and prereqs control between stages of the Guidance flow. \textbf{Done}.

\item Network graph controller. \textbf{Done}.
\end{itemize}

\section{Format Multivariate Time Series Data}
\label{sec:orgaf8a77a}

The program makes data preprocessing by inspecting data and giving recommendations through a hierarchical network graph navigation (3). The steps (5) related to this are:

\begin{itemize}
\item Load data. Data from multiple sources can include data synchronization techniques like aggregation, disgregation, or data alignment in terms of date, creating additional missing values.

\item Define appropriate data types and structures for time series.

\item Inspect data for missing values, addressing them through interpolation or imputation methods (4).

\item Stationarity testing with Augmented Dickey-Fuller test.

\item Look for characteristics that change the model assumptions, like exponential growth, periodicities, or ciclity patterns.

\item Transformation like a logarithmic or square root to stabilize variance and reduce skewness.

\item Data normalization by scaling data to enhance comparability.

\item Decomposition into a trend, seasonality, and residuals to isolate cyclic behavior.
\end{itemize}

\section{References}
\label{sec:orgf7a46e8}

\begin{enumerate}
\item \url{https://github.com/flordeluz/VisWeb}
\item Mahmood Al-khassaweneh, Mark Bronakowski, Esraa Al-Sharoa (2023). Multivariate and Dimensionality-Reduction-Based Machine Learning Techniques for Tumor Classification of RNA-Seq Data. Engineering, Computing and Mathematical Sciences, Lewis University, Romeoville, USA. Computer Engineering Department, Yarmouk University, Jordan. Electrical Engineering Department, Jordan University of Science and Technology, Jordan.
\item Stefan Gladisch, Heidrun Schumann, Christian Tominski. Navigation Recommendations for Exploring Hierarchical Graphs. Institute for Computer Science, University of Rostock, Germany.
\item \url{https://ch.mathworks.com/help/econ/multivariate-time-series-data-structures.html}
\item Can Zhou, Masami Fujiwara, William E. Grant (2016). Finding regulation among seemingly unregulated populations: a practical framework for analyzing multivariate population time series for their interactions. Springer Science Business Media New York. \url{https://www.researchgate.net/figure/Steps-in-analyzing-multivariate-time-series-data-a-After-plotting-each-individual-time\_fig2\_288179902}
\end{enumerate}

\clearpage
\section{Appendix}
\label{sec:org4b26bd4}

\subsection{Time Estimation Plan}
\label{sec:orge47bcdd}

\begin{itemize}
\item 2 months +4 backup weeks

\begin{longtable}{llllllllllllll}
\caption{Activities in weeks}
\\[0pt]
\cellcolor{gray!16} \textbf{Activities} & \cellcolor{gray!16} \textbf{1} & \cellcolor{gray!16} \textbf{2} & \cellcolor{gray!16} \textbf{3} & \cellcolor{gray!16} \textbf{4} & \cellcolor{gray!16} \textbf{5} & \cellcolor{gray!16} \textbf{6} & \cellcolor{gray!16} \textbf{7} & \cellcolor{gray!16} \textbf{8} & \cellcolor{gray!16} \textbf{9} & \cellcolor{gray!16} \textbf{10} & \cellcolor{gray!16} \textbf{11} & \cellcolor{gray!16} \textbf{12} & \cellcolor{gray!16} \textbf{13}\\[0pt]
\hline
\endfirsthead
\multicolumn{14}{l}{Continued from previous page} \\[0pt]
\hline

\cellcolor{gray!16} \textbf{Activities} & \cellcolor{gray!16} \textbf{1} & \cellcolor{gray!16} \textbf{2} & \cellcolor{gray!16} \textbf{3} & \cellcolor{gray!16} \textbf{4} & \cellcolor{gray!16} \textbf{5} & \cellcolor{gray!16} \textbf{6} & \cellcolor{gray!16} \textbf{7} & \cellcolor{gray!16} \textbf{8} & \cellcolor{gray!16} \textbf{9} & \cellcolor{gray!16} \textbf{10} & \cellcolor{gray!16} \textbf{11} & \cellcolor{gray!16} \textbf{12} & \cellcolor{gray!16} \textbf{13} \\[0pt]

\hline
\endhead
\hline\multicolumn{14}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
\cellcolor{gray!16}Backend DS & \cellcolor{blue!16} & \cellcolor{blue!16} &  &  &  &  &  &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}DS Integration &  & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} &  &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}Frontend DS &  &  &  & \cellcolor{blue!16} & \cellcolor{blue!16} & \cellcolor{blue!16} & \cellcolor{blue!16} &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}New ML Backend models &  &  & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} &  &  &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}Frontend new ML models &  &  &  &  &  & \cellcolor{blue!16} & \cellcolor{blue!16} & \cellcolor{blue!16} &  &  &  &  & \\[0pt]
\cellcolor{gray!16}Frontend UI changes &  &  &  &  &  &  & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32}\\[0pt]
\end{longtable}
\end{itemize}


\subsection{Testing functions}
\label{sec:org5511dfd}

Testing functions are executed to give advice when the system is processing the signal. In the case of nulls, it uses WMAPE to prioritize the imputation algorithms; the rest are selected by the majority because each algorithm evaluates the same. When testing, more than 50\% evaluate true and false otherwise. Trend algorithms do not detect; they just use decomposition to break the signal into its main components.

\begin{longtable}{lll}
\caption{Testing function modules}
\\[0pt]
\textbf{Module} & \textbf{Functions} & \textbf{Hint}\\[0pt]
\hline
\endfirsthead
\multicolumn{3}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{Module} & \textbf{Functions} & \textbf{Hint} \\[0pt]

\hline
\endhead
\hline\multicolumn{3}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
Nulls imputation & null\_values\_data(DF): bool & Nulls detection\\[0pt]
 & fill\_w\_meanmedian(*dict) & Rolling mean\\[0pt]
 & fill\_w\_decisiontree(*dict) & Decision tree regressor\\[0pt]
 & fill\_w\_gradientboosting(*dict) & Gradient boosting\\[0pt]
 & fill\_w\_locallyweighted(*dict) & Locally weighted regression\\[0pt]
 & fill\_w\_legendre(*dict) & Legendre polynomials\\[0pt]
 & fill\_w\_randomforest(*dict) & Random forest regressor\\[0pt]
 & fill\_w\_kneighbors(*dict) & kNN regression\\[0pt]
\hline
Cleaning & obtener\_ruido\_de(DF, int): bool & Standard deviation of noise\\[0pt]
 & obtener\_ruido\_cv(DF, float): bool & Noise covariance Kalman filter\\[0pt]
 & obtener\_outlier\_iqr(DF): bool & Interquartile range\\[0pt]
 & obtener\_outlier\_zscore(DF, int): bool & Z-score\\[0pt]
 & obtener\_outlier\_grubbs(DF, float): bool & Grubbs\\[0pt]
\hline
Normalization & obtener\_no\_patrones\_estacionalidad(DF, int): bool & Variance and noise\\[0pt]
 & obtener\_distribucion\_conocida(DF): bool & Deviation and log-normal\\[0pt]
\hline
Transformation & obtener\_no\_estacionariedad\_adf(DF, float): bool & Augmented Dickey-Fuller\\[0pt]
 & obtener\_no\_estacionariedad\_kpss(DF, float): bool & KPSS\\[0pt]
 & obtener\_comportamiento\_persistente\_hurst(DF): bool & Hurst\\[0pt]
\hline
Reduction & verificar\_correlacion\_pearson(DF, float): bool & Pearson correlation\\[0pt]
 & verificar\_correlacion\_spearman(DF, float): bool & Spearman correlation\\[0pt]
 & verificar\_correlacion\_kendall(DF, float): bool & Kendall correlation\\[0pt]
 & check\_multicollinearity(DF, int): bool & Multicollinearity test\\[0pt]
 & check\_dimensionality\_reduction\_pca(DF, float): bool & PCA\\[0pt]
 & check\_dimensionality\_reduction\_fa(DF, int, float): bool & Factor Analysis\\[0pt]
\hline
Decomposition & seasonality\_detection(Series, Array): LOB & Seasonal decompose\\[0pt]
 & trend\_detection(Series, Array): LOB & Seasonal decompose trending\\[0pt]
 & noise\_detection(Series, Array): LOB & Seasonal decompose residuals\\[0pt]
\end{longtable}


\subsection{Dictionary}
\label{sec:org88d0e5e}

The dictionary is dynamic, it grows at any algorithm or action made. Data is mostly held on Dataframes. Classes can retain information on the object like testing functions described in the previous subsection. The function contains the signal.

The following is an excerpt from the dictionary. It repeats for each loader class.

\medskip
\dirtree{%
.1 loaders (dict): Class type, one per dataset.
.2 aqp: GenLoader.
.2 brasil: GenLoader.
.2 btc: GenLoader.
.2 chiguata: GenLoader.
.2 india: GenLoader.
.2 madrid: GenLoader.
}

\medskip
\dirtree{%
.1 aqp: Main dataset.
.2 RM (dict): Imputation algorithm applied.
.3 MAJES (dict): Group or substation.
.4 PPT: function (Dimension).
.4 TNM: function (Dimension).
.4 TXM: function (Dimension).
.2 cache (dict).
.2 full: DataFrame: Full group or substation-treated data.
.2 iqr: DataFrame:  An entry of data for each executed algorithm.
.2 raw: DataFrame:  Full group or substations unchanged data.
.2 redo (dict).
.3 fill (dict).
.4 decisiontree: bool.
.4 gradientboosting: bool.
.4 kneighbors: bool.
.4 legendre: bool.
.4 locallyweighted: bool.
.4 meanmedian: bool.
.4 randomforest: bool.
.3 norm (dict).
.4 maxabs: bool.
.4 minmax: bool.
.4 robust: bool.
.4 standard: bool.
.3 outliers (dict).
.4 iqr: bool.
.4 sdv: bool.
.3 transform (dict).
.4 diff: bool.
.4 linear: bool.
.4 log: bool.
.4 quadratic: bool.
.4 sqrt: bool.
}


\subsection{Network coloring}
\label{sec:org2358d7d}

Color attributes are applied with information that comes from the backend as arrays.

The coloring algorithm works as each node is gray by default, then each node name that comes in the subprocess array overwrites as red then adjacent nodes on the right are colored green, and left or parents are colored red. If an array exception shows up it replaces the red node with orange. The activities array carries information about when a dialog will be launched if the user picks the activity. The array made-path carries the executed actions and allows the algorithm to overwrite the colors of the nodes as blue, the coloring process follows the graph adjacency theory.

\medskip
Trace:

\medskip
\textbf{Stage 1:}
\begin{verbatim}
    [ Algorithms priority ]:
    Array [ "Rolling Mean" ]

    [ Subprocesses ]:
    Array [ "Clean", "Nulls" ]
\end{verbatim}

\textbf{Stage 2:}
\begin{verbatim}
    [ Made Path ]:
    Array(3) [ "Clean", "Nulls", "Rolling Mean" ]
    
    [ Subprocesses ]:
    Array [ "Clean", "Outliers" ]
\end{verbatim}

\textbf{Stage 3:}
\begin{verbatim}
    [ Made Path ]:
    Array(6) [ "Clean", "Nulls", "Rolling Mean", "Clean", "Outliers",
    "Interquartile Range" ]
    
    [ Subprocesses ]:
    Array [ "DimRed" ]
    
    [ Activities ]:
    Array(3) [ "Multicollinearity Dim.Reduction=['PPT']", "PCA
    Dim.Reduction", "FA Dim.Reduction=['PPT']" ]
\end{verbatim}

\textbf{Stage 4:}
\begin{verbatim}
    [ Made Path ]:
    Array(8) [ "Clean", "Nulls", "Rolling Mean", "Clean", "Outliers",
    "Interquartile Range", "DimRed", "Factor Analysis" ]
    
    [ Subprocesses ]:
    Array [ "Analysis" ]
\end{verbatim}


\subsection{Change Log}
\label{sec:orgeff8805}

\begin{longtable}{lrlrrl}
\caption{List of patches by timestamp}
\\[0pt]
\textbf{A} & \textbf{Size} & \textbf{M} & \textbf{D} & \textbf{H} & \textbf{Patch}\\[0pt]
\hline
\endfirsthead
\multicolumn{6}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{A} & \textbf{Size} & \textbf{M} & \textbf{D} & \textbf{H} & \textbf{Patch} \\[0pt]

\hline
\endhead
\hline\multicolumn{6}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
-rw-r--r-- & 918794 & May & 22 & 01:38 & 20240522\_013818.diff\\[0pt]
-rw-r--r-- & 32116 & May & 24 & 01:39 & 20240524\_013931.diff\\[0pt]
-rw-r--r-- & 55590 & May & 24 & 18:17 & 20240524\_181739.diff\\[0pt]
-rw-r--r-- & 40553 & May & 27 & 01:09 & 20240527\_010932.diff\\[0pt]
-rw-r--r-- & 64068 & May & 28 & 01:45 & 20240528\_014458.diff\\[0pt]
-rw-r--r-- & 160383 & Jun & 2 & 03:04 & 20240602\_030410.diff\\[0pt]
-rw-r--r-- & 110627 & Jun & 4 & 02:45 & 20240604\_024500.diff\\[0pt]
-rw-r--r-- & 63591 & Jun & 5 & 00:46 & 20240605\_004621.diff\\[0pt]
-rw-r--r-- & 68193 & Jun & 6 & 02:31 & 20240606\_023122.diff\\[0pt]
-rw-r--r-- & 33434 & Jun & 7 & 03:04 & 20240607\_030453.diff\\[0pt]
-rw-r--r-- & 91512 & Jun & 8 & 01:59 & 20240608\_015902.diff\\[0pt]
-rw-r--r-- & 76847 & Jun & 10 & 18:28 & 20240610\_182855.diff\\[0pt]
-rw-r--r-- & 78312 & Jun & 16 & 01:54 & 20240616\_015442.diff\\[0pt]
-rw-r--r-- & 24940 & Jun & 17 & 02:10 & 20240617\_021030.diff\\[0pt]
-rw-r--r-- & 73776 & Jun & 18 & 01:05 & 20240618\_010542.diff\\[0pt]
-rw-r--r-- & 27856 & Jun & 18 & 10:28 & 20240618\_102832.diff\\[0pt]
-rw-r--r-- & 138469 & Jun & 25 & 02:45 & 20240625\_024532.diff\\[0pt]
-rw-r--r-- & 65443 & Jun & 26 & 02:59 & 20240626\_025901.diff\\[0pt]
-rw-r--r-- & 56919 & Jun & 27 & 00:56 & 20240627\_005611.diff\\[0pt]
-rw-r--r-- & 24307 & Jul & 1 & 00:37 & 20240701\_003738.diff\\[0pt]
-rw-r--r-- & 30011 & Jul & 3 & 16:51 & 20240703\_165116.diff\\[0pt]
-rw-r--r-- & 91786 & Jul & 7 & 01:55 & 20240707\_015505.diff\\[0pt]
-rw-r--r-- & 46989 & Jul & 9 & 01:33 & 20240709\_013313.diff\\[0pt]
-rw-r--r-- & 96392 & Jul & 12 & 01:51 & 20240712\_015153.diff\\[0pt]
-rw-r--r-- & 231634 & Jul & 17 & 01:38 & 20240717\_013822.diff\\[0pt]
-rw-rw-r-- & 1025353 & Jan & 5 & 16:25 & 20250105\_162546.diff\\[0pt]
-rw-rw-r-- & 52880 & Jan & 20 & 12:08 & 20250120\_120815.diff\\[0pt]
-rw-rw-r-- & 108172 & Jan & 21 & 23:19 & 20250121\_231951.diff\\[0pt]
-rw-rw-r-- & 48118 & Jan & 29 & 01:22 & 20250129\_012158.diff\\[0pt]
-rw-rw-r-- & 12685 & Jan & 30 & 10:32 & 20250130\_103221.diff\\[0pt]
-rw-rw-r-- & 36912 & Feb & 2 & 00:08 & 20250202\_000816.diff\\[0pt]
-rw-rw-r-- & 115526 & Feb & 10 & 09:59 & 20250210\_095953.diff\\[0pt]
\end{longtable}


\subsection{Backlog}
\label{sec:org380ec93}

\begin{longtable}{ll}
\caption{List of Backlog activities}
\\[0pt]
\hline
\textbf{Activity} & \textbf{Status}\\[0pt]
\hline
\endfirsthead
\multicolumn{2}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{Activity} & \textbf{Status} \\[0pt]

\hline
\endhead
\hline\multicolumn{2}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
Some parts of the code are setting -1 to complete null values. It needs & \emph{Closed}\\[0pt]
a review of the data with negative values like temperature. & \\[0pt]
\textbf{Resolution:} Used for initial visualization purposes. & \\[0pt]
\hline
View action buttons have to be reviewed in the front end. There are & \emph{Closed}\\[0pt]
buttons in the tree view requesting labels instead of the key data used & \\[0pt]
to look for. & \\[0pt]
\textbf{Resolution:} Remove action buttons on non-relevant leaf labels. & \\[0pt]
\hline
Integrate data and cache repositories with the front end. & \emph{Closed}\\[0pt]
\textbf{Resolution:} Implemented. & \\[0pt]
\hline
Does get-raw-data() revert .ds to .smo["raw"] when invoked in the & \emph{Closed}\\[0pt]
frontend? if so, uncomment the referred line in the get-raw-data(). & \\[0pt]
\textbf{Resolution:} We are using different structures inside .smo. & \\[0pt]
\hline
For large datasets the prediction time of null values pre-evaluation & \emph{Unsolved}\\[0pt]
goes like: & \\[0pt]
decisiontree    : too slow,  >   2 mins & \\[0pt]
kneighbors      : too slow,  >   2 mins & \\[0pt]
gradientboosting: slow,      >   1 min  < 2 mins & \\[0pt]
randomforest    : medium,    >  20 secs < 1 min & \\[0pt]
meanmedian      : regular,   <= 20 secs & \\[0pt]
locallyweighted : fast,      <=  6 secs & \\[0pt]
legendre        : very fast, <=  2 secs & \\[0pt]
\textbf{Resolution:} For large datasets predict with medium to fast algos. & \\[0pt]
\textbf{Author's note:} While testing large dataset stations of Madrid's data, & \\[0pt]
KNN gave better predictions. It is too risky to exclude slower algos & \\[0pt]
in the early stage of nulls' pre-evaluation. & \\[0pt]
\hline
\end{longtable}
\end{document}
