% Created 2025-02-13 Thu 03:46
% Intended LaTeX compiler: pdflatex
\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\usepackage{svg}
\author{Victor C, Flor De Luz}
\date{2024-05-20}
\title{DATA ANALYSIS AND VISUALIZATION OF MULTIVARIATE TIME SERIES SOFTWARE}
\hypersetup{
 pdfauthor={Victor C, Flor De Luz},
 pdftitle={DATA ANALYSIS AND VISUALIZATION OF MULTIVARIATE TIME SERIES SOFTWARE},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.3 (Org mode 9.6.15)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Introduction}
\label{sec:org66b71b1}

The project explores, analyzes, and integrates Vue.JS and Python code with different models for clean and data completion.

\section{Goals}
\label{sec:org673c4dc}

\begin{itemize}
\item Complete the code by integrating different Python models into the software project.

\item Data structure improvement to adapt new ML models for cleaning data and completion.

\item Technical documentation of the software.
\end{itemize}

\section{State of the art}
\label{sec:orgaee5a35}

\begin{itemize}
\item Source code of the project(1), with documentation to install and deploy the software. The software currently reads data and graphs time series. It has options for data completion through rolling mean and kNN and partially cleaning data alternatives. It tracks for changes while navigating through the Diagram Operator interface graphically. Integrates Radial Chart for time series cycles.

\item Self-documented notebook with multiple machine-learning techniques and their variants for data completion like Rolling Mean, Decision Trees, Stochastic Grading Boosting, Locally Weighted Regression, Legendre Polynomials Regression, Random Forest Regressor, k-nearest Neighbors. Includes removing features with no data at all. Computes a Dicky Fuller Stationarity Test. Automatically computes Weighted MAPE and R-Score + RMSE to detect and suggest which model fits better. Computes Autocorrelation, Fourier, and Hodrick Prescott to detect Cyclicity.
\end{itemize}

\section{Pre-implementation analysis}
\label{sec:orgedf6905}

\subsection{Multilayer design}
\label{sec:org67df0a8}

The critical aspects of the software are:

\begin{itemize}
\item Frontend developed in Vue.JS

\item Backend developed in Python

\item The data source layer primarily are CSV files \footnote{HDF5 detected}

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-069cc0f38b52fd29f3cd19de9f17d6d1e8ec9344.edraw}
\end{center}
\rmfamily \normalsize
\end{itemize}

\subsection{Frontend specifics}
\label{sec:orgd75c7b6}

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-1250ef5082f07210935c7515e20cafdd9150c370.edraw}
\end{center}
\rmfamily \normalsize

\subsection{Backend specifics}
\label{sec:org31e260e}

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-2de380ab2182abb59bc5043236fa1864057da655.edraw}
\end{center}
\rmfamily \normalsize

\subsection{Detected issues}
\label{sec:org20feccc}

\begin{itemize}
\item Data structures. It requires adapting the current data structures of the program to the new dictionary of the tested and trained models.

\item Exceptional cases. There are some treatments in the frontend code for specific data sources \footnote{TimeSeries.vue, temperature, and precipitation, line 308}. Those treatments will done in the backend.

\item The Radial Diagram component \footnote{Spiral.vue} hardcode for temperature and precipitation. We will rewrite the code to accept different datasets and features. Additionally, it is currently consuming the old backend\footnote{api/main.py}. The old backend has to be re-implemented in the new backend\footnote{new\textunderscore api/main.py}.
\end{itemize}

\section{{\bfseries\sffamily DONE} Implementation}
\label{sec:org93ba10a}

\subsection{Activities summary}
\label{sec:org5c82532}

\begin{itemize}
\item Adapt the Backend Data Structure.

\item Data Structure Integration. It will include saving a dictionary and allowing notebook compatibility.

\item Adapt the Frontend Data Structure.

\item Implement new ML models in the back end.

\item Adapt the front end for new ML models.

\item Frontend UI additional improvements.

\item Format multivariate time series data.
\end{itemize}


\subsection{{\bfseries\sffamily DONE} Activities execution}
\label{sec:org2c47333}

\subsubsection{Change log}
\label{sec:org559a4f7}

Tracking changes on the program will be done by using diff to create a patch component that allows the creation of a checkpoint and registering the changes by the size of modify or created code\footnote{Tracking log in the appendix}. The folder structure is:

\sffamily
\begin{longtable}{lll}
\caption{Change log folder structure}
\\[0pt]
\textbf{A} & \textbf{File System Structure} & \textbf{Description}\\[0pt]
\hline
\endfirsthead
\multicolumn{3}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{A} & \textbf{File System Structure} & \textbf{Description} \\[0pt]

\hline
\endhead
\hline\multicolumn{3}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
drwx & VisWeb-AlgoritmosLimpieza.orig & Original Source Code\\[0pt]
drwx & VisWeb-AlgoritmosLimpieza.incr & Incremental Checkpoint\\[0pt]
drwx & VisWeb-AlgoritmosLimpieza.diff & Diff/Patch Repository\\[0pt]
drwx & VisWeb-AlgoritmosLimpieza & Development Folder\\[0pt]
-r-- & checkpoint.lisp & The program for recording changes\\[0pt]
\end{longtable}
\rmfamily

\subsubsection{Backlog}
\label{sec:orgaeead3a}

The pending activities are in the appendix backlog section. These activities correspond to changes or reviews that depend on multiple program files around the software, and their resolutions will come on the project's timeline. Bugs will gradually fixed.

\subsubsection{{\bfseries\sffamily DONE} Backend data structures}
\label{sec:org8727723}

Programs adaptation in the new\textunderscore api.

\begin{itemize}
\item Creating a new MainloaderClass and its derived GenLoader class as a generic data loader that extends MainloaderClass\footnote{The new Class defines the notebook's data structure}. Methods' names remain unchanged to keep compatibility with the original code. \textbf{Created}.
\end{itemize}

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-b109f550576e09ad99add7f1f39939c296d2b388.edraw}
\end{center}
\rmfamily \normalsize

\begin{itemize}
\item The derived class GenLoader improves:

\begin{itemize}
\item The differences in the data structures returned by some Data Loaders. Those differences crash the Python kernel. \textbf{Fixed}.

\item Some old Classes' methods trunk data to 1000 rows for interprocess communication. \textbf{Fixed}.
\end{itemize}

\item Normalization will use MaxAbsScaler. Max scaler detected without considering negative values in some normalizations through the \emph{main.py} in the new\textunderscore api code. \textbf{Fixed}.

\item The GenLoader-derived class allows different datasets to load. It returns a new loader object with the needed structure. It loads the data from the source when the constructor creates an instance of the class. The main program will map correctly the different datasets when the front end requires it. \textbf{Implemented}.

\item The front end becomes slow with too much data. A resample by day solves the issue. Internally, it will keep the original data, and it will transform the data without resampling. The resamples are just for visualization purposes. \textbf{Fixed}.

\item Code refactoring is mandatory\footnote{Testing functions in the appendix}. Some functions have no real input for data observations and use a random dataset created inside the function. Other functions presented incomplete treatment. Dimensionality reduction rewritten functions (2). \textbf{Implemented}.

\item Duplicated and non-relevant functions have been removed. \textbf{Fixed}.

\item The transformations will be packed in the MainredoClass and its derived class GenRedo, where the methods will be different techniques to clean and complete data. \textbf{Created}.
\end{itemize}

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-41552ea8ba76b31fd563f0a168fb2c7cc54f6954.edraw}
\end{center}
\rmfamily \normalsize

\subsubsection{{\bfseries\sffamily DONE} Backend new specifications}
\label{sec:org3489a77}

\begin{enumerate}
\item {\bfseries\sffamily DONE} Classes and modules
\label{sec:org151e4fe}

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-7390a9932dd2cb3129ada2c263fda3d0172031ac.edraw}
\end{center}
\rmfamily \normalsize

\item {\bfseries\sffamily DONE} Sequence diagram
\label{sec:org1f37bba}

\sffamily \scriptsize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-abeb00b5b06a1ab332d0fe6ab7b8bcf777f1b970.edraw}
\end{center}
\rmfamily \normalsize
\end{enumerate}

\subsubsection{{\bfseries\sffamily DONE} Backend new data structure and frontend integration}
\label{sec:org7485a21}

The initial state of the front end had two APIs. All the methods to grab data were migrated and transformed in the new API, leaving just one centralized processing Python API.

The main structure of the front end has not changed too much. The most relevant changes were in coding because the program was created for Peruvian temperature and precipitation datasets.

\sffamily \footnotesize
\begin{center}
\includesvg[width=.9\linewidth]{link-data-2622180df0d470ec8299135efd823dc950ac899d.edraw}
\end{center}
\rmfamily \normalsize

\subsubsection{Data repository and cache}
\label{sec:org7016b60}

The backend will support dataset uploading. The structure has been defined on folders as an ID inside a .data repository. It will allow us to keep the data treatment progress in folders with the same ID inside a .cache repository.

\section{{\bfseries\sffamily DONE} Requirements}
\label{sec:orgc8da40d}

\subsection{{\bfseries\sffamily DONE} Software}
\label{sec:orgb40c177}

\begin{itemize}
\item Python 3.12.7

\item NodeJS 18.19.1

\item GNU/Linux distribution with kernel 5.15.19 or superior

\item Windows has not been tested, but it may work
\end{itemize}

\subsection{{\bfseries\sffamily DONE} Hardware}
\label{sec:orgbbfde63}

\begin{itemize}
\item Processor AMD64 or x86\textsubscript{64} architecture

\item 16GB RAM

\item 32GB Swap

\item 64GB SSD (128GB SSD Recommended)

\item GPU (Optional)
\end{itemize}

\section{{\bfseries\sffamily DONE} Visual Analytics Guidance Development}
\label{sec:orga3ac269}

\subsection{{\bfseries\sffamily DONE} Spiral diagram analysis improvement}
\label{sec:orge011339}

\begin{itemize}
\item Compute segments and timespan for the Spiral. \textbf{Done}.

\item Multiple time series integration. \textbf{Done}.

\item Fix polygon coordinates when there are more than three dimensions. \textbf{Done}.
\end{itemize}

\subsection{{\bfseries\sffamily DONE} Independent section for statistics}
\label{sec:orgfe42b25}

\begin{itemize}
\item Statistics of the time series (size, nulls, type of columns, or dimensions). \textbf{Done}.

\item Distribution type of the time series (e.g., normal). \textbf{Done}.

\item Outliers. \textbf{Done}.

\item Correlation matrix. \textbf{Done}.
\end{itemize}

\subsection{{\bfseries\sffamily DONE} Flow diagram improvement}
\label{sec:org476c6b6}

\begin{itemize}
\item Data cleaning, Normalization and transformation, and Time-series behavior. \textbf{Done}.

\item Stoppers and prereqs control between stages of the Guidance flow. \textbf{Done}.

\item Network graph controller. \textbf{Done}.
\end{itemize}

\section{{\bfseries\sffamily DONE} Format Multivariate Time Series Data}
\label{sec:org1b0d6a5}

The program makes data preprocessing by inspecting data and giving recommendations through a hierarchical network graph navigation (3). The steps (5) related to this are:

\begin{itemize}
\item Load data. Data from multiple sources can include data synchronization techniques like aggregation, disgregation, or data alignment in terms of date, creating additional missing values.

\item Define appropriate data types and structures for time series.

\item Inspect data for missing values, addressing them through interpolation or imputation methods (4).

\item Stationarity testing with Augmented Dickey-Fuller test.

\item Look for characteristics that change the model assumptions, like exponential growth, periodicities, or ciclity patterns.

\item Transformation like a logarithmic or square root to stabilize variance and reduce skewness.

\item Data normalization by scaling data to enhance comparability.

\item Decomposition into a trend, seasonality, and residuals to isolate cyclic behavior.
\end{itemize}

\section{References}
\label{sec:org09f9ef0}

\begin{enumerate}
\item \url{https://github.com/flordeluz/VisWeb}
\item Mahmood Al-khassaweneh, Mark Bronakowski, Esraa Al-Sharoa (2023). Multivariate and Dimensionality-Reduction-Based Machine Learning Techniques for Tumor Classification of RNA-Seq Data. Engineering, Computing and Mathematical Sciences, Lewis University, Romeoville, USA. Computer Engineering Department, Yarmouk University, Jordan. Electrical Engineering Department, Jordan University of Science and Technology, Jordan.
\item Stefan Gladisch, Heidrun Schumann, Christian Tominski. Navigation Recommendations for Exploring Hierarchical Graphs. Institute for Computer Science, University of Rostock, Germany.
\item \url{https://ch.mathworks.com/help/econ/multivariate-time-series-data-structures.html}
\item Can Zhou, Masami Fujiwara, William E. Grant (2016). Finding regulation among seemingly unregulated populations: a practical framework for analyzing multivariate population time series for their interactions. Springer Science Business Media New York. \url{https://www.researchgate.net/figure/Steps-in-analyzing-multivariate-time-series-data-a-After-plotting-each-individual-time\_fig2\_288179902}
\end{enumerate}

\clearpage
\section{Appendix}
\label{sec:orga8b7b62}

\subsection{Time Estimation Plan}
\label{sec:orga6b5254}

\begin{itemize}
\item 2 months +4 backup weeks

\sffamily
\begin{longtable}{llllllllllllll}
\caption{Activities in weeks}
\\[0pt]
\cellcolor{gray!16} \textbf{Activities} & \cellcolor{gray!16} \textbf{1} & \cellcolor{gray!16} \textbf{2} & \cellcolor{gray!16} \textbf{3} & \cellcolor{gray!16} \textbf{4} & \cellcolor{gray!16} \textbf{5} & \cellcolor{gray!16} \textbf{6} & \cellcolor{gray!16} \textbf{7} & \cellcolor{gray!16} \textbf{8} & \cellcolor{gray!16} \textbf{9} & \cellcolor{gray!16} \textbf{10} & \cellcolor{gray!16} \textbf{11} & \cellcolor{gray!16} \textbf{12} & \cellcolor{gray!16} \textbf{13}\\[0pt]
\hline
\endfirsthead
\multicolumn{14}{l}{Continued from previous page} \\[0pt]
\hline

\cellcolor{gray!16} \textbf{Activities} & \cellcolor{gray!16} \textbf{1} & \cellcolor{gray!16} \textbf{2} & \cellcolor{gray!16} \textbf{3} & \cellcolor{gray!16} \textbf{4} & \cellcolor{gray!16} \textbf{5} & \cellcolor{gray!16} \textbf{6} & \cellcolor{gray!16} \textbf{7} & \cellcolor{gray!16} \textbf{8} & \cellcolor{gray!16} \textbf{9} & \cellcolor{gray!16} \textbf{10} & \cellcolor{gray!16} \textbf{11} & \cellcolor{gray!16} \textbf{12} & \cellcolor{gray!16} \textbf{13} \\[0pt]

\hline
\endhead
\hline\multicolumn{14}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
\cellcolor{gray!16}Backend DS & \cellcolor{blue!16} & \cellcolor{blue!16} &  &  &  &  &  &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}DS Integration &  & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} &  &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}Frontend DS &  &  &  & \cellcolor{blue!16} & \cellcolor{blue!16} & \cellcolor{blue!16} & \cellcolor{blue!16} &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}New ML Backend models &  &  & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} &  &  &  &  &  &  &  & \\[0pt]
\cellcolor{gray!16}Frontend new ML models &  &  &  &  &  & \cellcolor{blue!16} & \cellcolor{blue!16} & \cellcolor{blue!16} &  &  &  &  & \\[0pt]
\cellcolor{gray!16}Frontend UI changes &  &  &  &  &  &  & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32} & \cellcolor{blue!32}\\[0pt]
\end{longtable}
\rmfamily
\end{itemize}

\subsection{Testing functions}
\label{sec:orgf1d8f35}

Testing functions are executed to give advice when the system is processing the signal. In the case of nulls, it uses WMAPE to prioritize the imputation algorithms, the rest are selected by the majority because each algorithm evaluates the same. When testing, more than 50\% evaluate true and false otherwise. Trend algorithms do not detect, just use decomposition to break the signal in its main components.

\sffamily
\begin{longtable}{ll}
\caption{Testing function modules}
\\[0pt]
\textbf{Module} & \textbf{Functions}\\[0pt]
\hline
\endfirsthead
\multicolumn{2}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{Module} & \textbf{Functions} \\[0pt]

\hline
\endhead
\hline\multicolumn{2}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
Nulls imputation & null\textunderscore values\textunderscore data(DF): bool\\[0pt]
 & fill\textunderscore w\textunderscore meanmedian(*dict)\\[0pt]
 & fill\textunderscore w\textunderscore decisiontree(*dict)\\[0pt]
 & fill\textunderscore w\textunderscore gradientboosting(*dict)\\[0pt]
 & fill\textunderscore w\textunderscore locallyweighted(*dict)\\[0pt]
 & fill\textunderscore w\textunderscore legendre(*dict)\\[0pt]
 & fill\textunderscore w\textunderscore randomforest(*dict)\\[0pt]
 & fill\textunderscore w\textunderscore kneighbors(*dict)\\[0pt]
\hline
Cleaning & obtener\textunderscore ruido\textunderscore de(DF, int): bool\\[0pt]
 & obtener\textunderscore ruido\textunderscore cv(DF, float): bool\\[0pt]
 & obtener\textunderscore outlier\textunderscore iqr(DF): bool\\[0pt]
 & obtener\textunderscore outlier\textunderscore zscore(DF, int): bool\\[0pt]
 & obtener\textunderscore outlier\textunderscore grubbs(DF, float): bool\\[0pt]
\hline
Normalization & obtener\textunderscore no\textunderscore patrones\textunderscore estacionalidad(DF, int): bool\\[0pt]
 & obtener\textunderscore distribucion\textunderscore conocida(DF): bool\\[0pt]
\hline
Transformation & obtener\textunderscore no\textunderscore estacionariedad\textunderscore adf(DF, float): bool\\[0pt]
 & obtener\textunderscore no\textunderscore estacionariedad\textunderscore kpss(DF, float): bool\\[0pt]
 & obtener\textunderscore comportamiento\textunderscore persistente\textunderscore hurst(DF): bool\\[0pt]
\hline
Reduction & verificar\textunderscore correlacion\textunderscore pearson(DF, float): bool\\[0pt]
 & verificar\textunderscore correlacion\textunderscore spearman(DF, float): bool\\[0pt]
 & verificar\textunderscore correlacion\textunderscore kendall(DF, float): bool\\[0pt]
 & check\textunderscore multicollinearity(DF, int): bool\\[0pt]
 & check\textunderscore dimensionality\textunderscore reduction\textunderscore pca(DF, float): bool\\[0pt]
 & check\textunderscore dimensionality\textunderscore reduction\textunderscore fa(DF, int, float): bool\\[0pt]
\hline
Decomposition & seasonality\textunderscore detection(Series, Array): LOB\\[0pt]
 & trend\textunderscore detection(Series, Array): LOB\\[0pt]
 & noise\textunderscore detection(Series, Array): LOB\\[0pt]
\end{longtable}
\rmfamily

\subsection{Dictionary}
\label{sec:orgdbe4888}

The dictionary is dynamic, it grows at any algorithm or action made. Data is mostly held on Dataframes. Classes can retain information on the object like testing functions described in the previous subsection. The function contains the signal.

The following is an excerpt from the dictionary. It repeats for each loader class.

\begin{verbatim}
loaders (dict)
|-- aqp: GenLoader
|-- brasil: GenLoader
|-- btc: GenLoader
|-- chiguata: GenLoader
|-- india: GenLoader
\-- madrid: GenLoader

aqp
|-- RM (dict)
|   \-- MAJES (dict)
|       |-- PPT: function
|       |-- TNM: function
|       \-- TXM: function
|-- cache (dict)
|-- full: DataFrame
|-- iqr: DataFrame
|-- raw: DataFrame
\-- redo (dict)
    |-- fill (dict)
    |   |-- decisiontree: bool
    |   |-- gradientboosting: bool
    |   |-- kneighbors: bool
    |   |-- legendre: bool
    |   |-- locallyweighted: bool
    |   |-- meanmedian: bool
    |   \-- randomforest: bool
    |-- norm (dict)
    |   |-- maxabs: bool
    |   |-- minmax: bool
    |   |-- robust: bool
    |   \-- standard: bool
    |-- outliers (dict)
    |   |-- iqr: bool
    |   \-- sdv: bool
    \-- transform (dict)
        |-- diff: bool
        |-- linear: bool
        |-- log: bool
        |-- quadratic: bool
        \-- sqrt: bool
\end{verbatim}


\subsection{Network coloring}
\label{sec:org63e6dad}

Color attributes are applied with information that comes from the backend as arrays.

The coloring algorithm works as each node is gray by default, then each node name that comes in the subprocess array overwrites as red then adjacent nodes on the right are colored green, and left or parents are colored red. If an array exception shows up it replaces the red node with orange. The activities array carries information about when a dialog will be launched if the user picks the activity. The array made-path carries the executed actions and allows the algorithm to overwrite the colors of the nodes as blue, the coloring process follows the graph adjacency theory.

Trace:

\textbf{Stage 1:}
\begin{verbatim}
[ Algorithms priority ]:  
Array [ "Rolling Mean" ]

[ Subprocesses ]: 
Array [ "Clean", "Nulls" ]
\end{verbatim}

\textbf{Stage 2:}
\begin{verbatim}
[ Made Path ]:  
Array(3) [ "Clean", "Nulls", "Rolling Mean" ]

[ Subprocesses ]: 
Array [ "Clean", "Outliers" ]
\end{verbatim}

\textbf{Stage 3:}
\begin{verbatim}
[ Made Path ]:  
Array(6) [ "Clean", "Nulls", "Rolling Mean", "Clean", "Outliers", "Interquartile Range" ]

[ Subprocesses ]: 
Array [ "DimRed" ]

[ Activities ]:
Array(3) [ "Multicollinearity Dim.Reduction=['PPT']", "PCA Dim.Reduction", "FA Dim.Reduction=['PPT']" ]
\end{verbatim}

\textbf{Stage 4:}
\begin{verbatim}
[ Made Path ]:  
Array(8) [ "Clean", "Nulls", "Rolling Mean", "Clean", "Outliers", "Interquartile Range", "DimRed", "Factor Analysis" ]

[ Subprocesses ]: 
Array [ "Analysis" ]
\end{verbatim}


\subsection{Change Log}
\label{sec:orga7abd76}

\sffamily
\begin{longtable}{lrlrrl}
\caption{List of patches by timestamp}
\\[0pt]
\textbf{A} & \textbf{Size} & \textbf{M} & \textbf{D} & \textbf{H} & \textbf{Patch}\\[0pt]
\hline
\endfirsthead
\multicolumn{6}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{A} & \textbf{Size} & \textbf{M} & \textbf{D} & \textbf{H} & \textbf{Patch} \\[0pt]

\hline
\endhead
\hline\multicolumn{6}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
-rw-r--r-- & 918794 & May & 22 & 01:38 & 20240522-013818.diff\\[0pt]
-rw-r--r-- & 32116 & May & 24 & 01:39 & 20240524-013931.diff\\[0pt]
-rw-r--r-- & 55590 & May & 24 & 18:17 & 20240524-181739.diff\\[0pt]
-rw-r--r-- & 40553 & May & 27 & 01:09 & 20240527-010932.diff\\[0pt]
-rw-r--r-- & 64068 & May & 28 & 01:45 & 20240528-014458.diff\\[0pt]
-rw-r--r-- & 160383 & Jun & 2 & 03:04 & 20240602-030410.diff\\[0pt]
-rw-r--r-- & 110627 & Jun & 4 & 02:45 & 20240604-024500.diff\\[0pt]
-rw-r--r-- & 63591 & Jun & 5 & 00:46 & 20240605-004621.diff\\[0pt]
-rw-r--r-- & 68193 & Jun & 6 & 02:31 & 20240606-023122.diff\\[0pt]
-rw-r--r-- & 33434 & Jun & 7 & 03:04 & 20240607-030453.diff\\[0pt]
-rw-r--r-- & 91512 & Jun & 8 & 01:59 & 20240608-015902.diff\\[0pt]
-rw-r--r-- & 76847 & Jun & 10 & 18:28 & 20240610-182855.diff\\[0pt]
-rw-r--r-- & 78312 & Jun & 16 & 01:54 & 20240616-015442.diff\\[0pt]
-rw-r--r-- & 24940 & Jun & 17 & 02:10 & 20240617-021030.diff\\[0pt]
-rw-r--r-- & 73776 & Jun & 18 & 01:05 & 20240618-010542.diff\\[0pt]
-rw-r--r-- & 27856 & Jun & 18 & 10:28 & 20240618-102832.diff\\[0pt]
-rw-r--r-- & 138469 & Jun & 25 & 02:45 & 20240625-024532.diff\\[0pt]
-rw-r--r-- & 65443 & Jun & 26 & 02:59 & 20240626-025901.diff\\[0pt]
-rw-r--r-- & 56919 & Jun & 27 & 00:56 & 20240627-005611.diff\\[0pt]
-rw-r--r-- & 24307 & Jul & 1 & 00:37 & 20240701-003738.diff\\[0pt]
-rw-r--r-- & 30011 & Jul & 3 & 16:51 & 20240703-165116.diff\\[0pt]
-rw-r--r-- & 91786 & Jul & 7 & 01:55 & 20240707-015505.diff\\[0pt]
-rw-r--r-- & 46989 & Jul & 9 & 01:33 & 20240709-013313.diff\\[0pt]
-rw-r--r-- & 96392 & Jul & 12 & 01:51 & 20240712-015153.diff\\[0pt]
-rw-r--r-- & 231634 & Jul & 17 & 01:38 & 20240717-013822.diff\\[0pt]
-rw-rw-r-- & 1025353 & Jan & 5 & 16:25 & 20250105-162546.diff\\[0pt]
-rw-rw-r-- & 52880 & Jan & 20 & 12:08 & 20250120-120815.diff\\[0pt]
-rw-rw-r-- & 108172 & Jan & 21 & 23:19 & 20250121-231951.diff\\[0pt]
-rw-rw-r-- & 48118 & Jan & 29 & 01:22 & 20250129-012158.diff\\[0pt]
-rw-rw-r-- & 12685 & Jan & 30 & 10:32 & 20250130-103221.diff\\[0pt]
-rw-rw-r-- & 36912 & Feb & 2 & 00:08 & 20250202-000816.diff\\[0pt]
-rw-rw-r-- & 115526 & Feb & 10 & 09:59 & 20250210-095953.diff\\[0pt]
\end{longtable}
\rmfamily

\subsection{Backlog}
\label{sec:orge8b75e7}

\sffamily
\begin{longtable}{ll}
\caption{List of Backlog activities}
\\[0pt]
\hline
\textbf{Activity} & \textbf{Status}\\[0pt]
\hline
\endfirsthead
\multicolumn{2}{l}{Continued from previous page} \\[0pt]
\hline

\textbf{Activity} & \textbf{Status} \\[0pt]

\hline
\endhead
\hline\multicolumn{2}{r}{Continued on next page} \\
\endfoot
\endlastfoot
\hline
Some parts of the code are setting -1 to complete null values. It needs & \emph{Closed}\\[0pt]
a review of the data with negative values like temperature. & \\[0pt]
\textbf{Resolution:} Used for initial visualization purposes. & \\[0pt]
\hline
View action buttons have to be reviewed in the front end. There are & \emph{Closed}\\[0pt]
buttons in the tree view requesting labels instead of the key data used & \\[0pt]
to look for. & \\[0pt]
\textbf{Resolution:} Remove action buttons on non-relevant leaf labels. & \\[0pt]
\hline
Integrate data and cache repositories with the front end. & \emph{Closed}\\[0pt]
\textbf{Resolution:} Implemented. & \\[0pt]
\hline
Does get-raw-data() revert .ds to .smo["raw"] when invoked in the & \emph{Closed}\\[0pt]
frontend? if so, uncomment the referred line in the get-raw-data(). & \\[0pt]
\textbf{Resolution:} We are using different structures inside .smo. & \\[0pt]
\hline
For large datasets the prediction time of null values pre-evaluation & \emph{Unsolved}\\[0pt]
goes like: & \\[0pt]
decisiontree    : too slow,  >   2 mins & \\[0pt]
kneighbors      : too slow,  >   2 mins & \\[0pt]
gradientboosting: slow,      >   1 min  < 2 mins & \\[0pt]
randomforest    : medium,    >  20 secs < 1 min & \\[0pt]
meanmedian      : regular,   <= 20 secs & \\[0pt]
locallyweighted : fast,      <=  6 secs & \\[0pt]
legendre        : very fast, <=  2 secs & \\[0pt]
\textbf{Resolution:} For large datasets predict with medium to fast algos. & \\[0pt]
\textbf{Author's note:} While testing large dataset stations of Madrid's data, & \\[0pt]
KNN gave better predictions. It is too risky to exclude slower algos & \\[0pt]
in the early stage of nulls' pre-evaluation. & \\[0pt]
\hline
\end{longtable}
\rmfamily
\end{document}
